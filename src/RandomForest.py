# ============================================
# ğŸŒ² Entrenamiento optimizado con Random Forest
# ============================================

from sklearn.ensemble import RandomForestClassifier
from typing import Dict
import numpy as np

def train_randomForest(values: Dict):
    """
    ğŸš€ Entrena un modelo Random Forest con hiperparÃ¡metros optimizados
    para detecciÃ³n de exoplanetas o problemas con desbalance de clases.

    ParÃ¡metros clave:
    - n_estimators: nÃºmero de Ã¡rboles en el bosque
    - max_depth: controla la complejidad del modelo
    - min_samples_split / min_samples_leaf: previenen overfitting
    - max_features: fracciÃ³n de caracterÃ­sticas por Ã¡rbol (sqrt = recomendada)
    - class_weight='balanced': ajusta el peso de clases automÃ¡ticamente
    """

    # ===============================
    # ğŸ“¦ Cargar datos de entrenamiento
    # ===============================
    X_train = values["X_train"]
    y_train = values["y_train"]

    # ===============================
    # âš–ï¸ Calcular distribuciÃ³n de clases
    # ===============================
    unique, counts = np.unique(y_train, return_counts=True)
    class_dist = dict(zip(unique, counts))

    print(f"ğŸ“Š Clases detectadas: {class_dist}")
    if len(unique) > 1:
        ratio = counts.min() / counts.max()
        print(f"âš–ï¸ Balance entre clases: {ratio:.2f}")
    else:
        print("âš ï¸ Solo se detectÃ³ una clase. Entrenamiento sin balanceo.")
    
    # ===============================
    # ğŸ¯ ConfiguraciÃ³n de hiperparÃ¡metros
    # ===============================
    params = {
        'n_estimators': 300,           # ğŸŒ² MÃ¡s Ã¡rboles para mayor robustez
        'max_depth': 20,               # ğŸŒŒ Profundidad controlada para evitar overfitting
        'min_samples_split': 8,        # ğŸª¶ DivisiÃ³n mÃ­nima de nodos
        'min_samples_leaf': 3,         # ğŸƒ NÃºmero mÃ­nimo de muestras por hoja
        'max_features': 'sqrt',        # âš™ï¸ Aleatoriedad saludable por Ã¡rbol
        'bootstrap': True,             # ğŸ” Muestreo con reemplazo (clave en RF)
        'class_weight': 'balanced',    # âš–ï¸ Ajuste automÃ¡tico por frecuencia de clases
        'random_state': 42,            # ğŸ§­ Reproducibilidad
        'n_jobs': -1                   # ğŸ’» ParalelizaciÃ³n total
    }

    # ===============================
    # ğŸ§  Crear y entrenar el modelo
    # ===============================
    clf = RandomForestClassifier(**params)

    clf.fit(X_train, y_train)
    print(f"âœ… Random Forest entrenado con {params['n_estimators']} Ã¡rboles y profundidad mÃ¡xima {params['max_depth']}")
    print(f"ğŸŒ² Cada Ã¡rbol ve ~{params['max_features']} de las caracterÃ­sticas disponibles")

    # ===============================
    # ğŸ“ˆ Importancia de caracterÃ­sticas
    # ===============================
    try:
        importances = clf.feature_importances_
        top_idx = np.argsort(importances)[::-1][:5]
        print("\nğŸ’¡ CaracterÃ­sticas mÃ¡s importantes:")
        for i in top_idx:
            print(f" - {X_train.columns[i]}: {importances[i]:.4f}")
    except Exception as e:
        print(f"âš ï¸ No se pudieron mostrar importancias: {e}")

    return clf
